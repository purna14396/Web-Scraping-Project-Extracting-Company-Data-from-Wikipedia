# -*- coding: utf-8 -*-
"""WeB Scraping Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1136HYwll5H5qnz3KzGyeEwgM2bRL21Cy

# **Scraping Website Data**
"""

from bs4 import BeautifulSoup
import requests

url = "https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue"

page = requests.get(url)

soup = BeautifulSoup(page.text, "html")

print(soup.prettify())

# we can also use this by class selector :
# soup.find('table' , class_= "wikitable sortable")

table = soup.find_all('table')[0]

print(table)

# headers = table.find_all('th')
# print(headers)
# for i in headers:
#     print(i.text.strip())

world_table_titles = [title.text.strip() for title in table.find_all("th")]

print(world_table_titles)

"""# **Storing Into The Files Using Pandas**"""

import pandas as pd

df = pd.DataFrame( columns = world_table_titles )
df

# column_data = table.find_all('tr')

# world_row_data = []

# for row in column_data:
#     row_data = row.find_all('td')
#     world_row_data.append([data.text.strip() for data in row_data])

# for row in world_row_data:

#     print(row)

column_data = table.find_all('tr')

for row in column_data[1:]:
    row_data = row.find_all('td')
    individual_row_data = [data.text.strip() for data in row_data]
    # print(individual_row_data)

    length = len(df)
    df.loc[length] = individual_row_data

df

df.to_csv(r"/content/sample_data/companies.csv" , index = False)

